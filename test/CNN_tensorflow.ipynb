{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyper-parameters\n",
    "RATIO = 0.8   ## What ratio of total lines used for training (the rest is for validation)\n",
    "FC_SIZE = 512\n",
    "LR = 1e-4\n",
    "L2_REG = 0.01\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 200\n",
    "KERNEL_SIZE = 5     ## One side (square)\n",
    "FEATURE_MAP1 = 32   ## First conv layer feature maps\n",
    "FEATURE_MAP2 = 64  ## Second conv layer feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/train.csv\") as f:\n",
    "    lines_all = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [np.array(line.split(\",\"), dtype='float32') for line in lines_all[1:]]\n",
    "#random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_images = []\n",
    "training_labels = []\n",
    "validation_images = []\n",
    "validation_labels = []\n",
    "for line in lines[:int(len(lines) * RATIO)]:\n",
    "    one_hot = np.zeros((10), dtype='float32')\n",
    "    one_hot[int(line[0])] = 1.0\n",
    "    training_labels.append(one_hot)\n",
    "    training_images.append(line[1:])\n",
    "training_set = [training_images, training_labels]\n",
    "for line in lines[int(len(lines) * RATIO):]:\n",
    "\tone_hot = np.zeros((10), dtype='float32')\n",
    "\tone_hot[int(line[0])] = 1.0\n",
    "\tvalidation_labels.append(one_hot)\n",
    "\tvalidation_images.append(line[1:])\n",
    "validation_set = [validation_images, validation_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## See first n random digit with their labels\n",
    "n = 15\n",
    "fig = plt.figure()\n",
    "for i in xrange(n):\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(training_labels[i].nonzero()[0][0])  ## Label\n",
    "    plt.imshow(training_images[i].reshape((28, 28)), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Constants\n",
    "TRAINING_SIZE = int(len(lines) * RATIO)\n",
    "VALIDATION_SIZE = len(lines) - TRAINING_SIZE\n",
    "TRAINING_BATCH = TRAINING_SIZE / BATCH_SIZE\n",
    "VALIDATION_BATCH = VALIDATION_SIZE / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print TRAINING_SIZE, VALIDATION_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create Batches ##\n",
    "training_batches = []   ## [[[batch1_images], [batch1_labels]], [[batch2_images], [batch2_labels]], ... ]\n",
    "for i in range(0, TRAINING_SIZE, BATCH_SIZE):\n",
    "    training_batch_image = training_set[0][i:i + BATCH_SIZE]\n",
    "    training_batch_label = training_set[1][i:i + BATCH_SIZE]\n",
    "    training_batches.append([training_batch_image, training_batch_label])\n",
    "    \n",
    "validation_batches = []\n",
    "for i in range(0, VALIDATION_SIZE, BATCH_SIZE):\n",
    "    validation_batch_image = validation_set[0][i:i + BATCH_SIZE]\n",
    "    validation_batch_label = validation_set[1][i:i + BATCH_SIZE]\n",
    "    validation_batches.append([validation_batch_image, validation_batch_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def weight_variable(shape):\n",
    "#  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "#  return tf.Variable(initial)\n",
    "def weight_variable(n_inputs, n_outputs):\n",
    "    return tf.Variable(xavier_init(n_inputs, n_outputs))\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Input, output vectors\n",
    "x = tf.placeholder(tf.float32, [None, 784])  ## Here 'None' means that a dimension can be of any length\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])  ## Correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 1st conv layer\n",
    "## [Kernel size1, Kernel size2, first layer channel, second layer channel channel]\n",
    "#W_conv1 = weight_variable([KERNEL_SIZE, KERNEL_SIZE, 1, FEATURE_MAP1])\n",
    "\n",
    "#b_conv1 = bias_variable([FEATURE_MAP1])\n",
    "with tf.variable_scope(\"layer1\"):\n",
    "    W_conv1 = tf.get_variable('W_conv1', shape=(KERNEL_SIZE, KERNEL_SIZE, 1, FEATURE_MAP1), \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_conv1 = tf.get_variable('b_conv1', shape=(FEATURE_MAP1),\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "# [.., shape1, shape2, channel]\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2nd conv layer\n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    W_conv2 = tf.get_variable('W_conv2', shape=(KERNEL_SIZE, KERNEL_SIZE, FEATURE_MAP1, FEATURE_MAP2), \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_conv2 = tf.get_variable('b_conv1', shape=(FEATURE_MAP2),\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "#W_conv2 = weight_variable([KERNEL_SIZE, KERNEL_SIZE, FEATURE_MAP1, FEATURE_MAP2])\n",
    "#b_conv2 = bias_variable([FEATURE_MAP2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fully connected layer\n",
    "\n",
    "## Image shape halves twice. (28x28) -> (14x14) -> (7x7) by max_pool_2x2\n",
    "## Conv. layer does not change image size because of padding='SAME'\n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    W_fc1 = tf.get_variable('W_fc1', shape=(7 * 7 * FEATURE_MAP2, FC_SIZE), \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = tf.get_variable('b_fc1', shape=(FC_SIZE),\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "#W_fc1 = weight_variable([7 * 7 * FEATURE_MAP2, FC_SIZE])  \n",
    "#b_fc1 = bias_variable([FC_SIZE])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * FEATURE_MAP2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Softmax output\n",
    "with tf.variable_scope(\"layer4\"):\n",
    "    W_fc2 = tf.get_variable('W_fc1', shape=(FC_SIZE, 10), \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc2 = tf.get_variable('b_fc2', shape=(10),\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "#W_fc2 = weight_variable([FC_SIZE, 10])\n",
    "#b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(LR).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ##############\n",
    "    ## Training ##\n",
    "    ##############\n",
    "\n",
    "    #saver.restore(sess, \"../model/MLP_tensorflow/MLP_tensorflow.ckpt\")\n",
    "    #print(\"Model restored.\")\n",
    "\n",
    "    training_accuracy = 0.0\n",
    "    t0 = time.time()\n",
    "    for i in xrange(EPOCH * TRAINING_BATCH):\n",
    "        \n",
    "        j = i % TRAINING_BATCH\n",
    "        sess.run(train_step, feed_dict={x: training_batches[j][0], y_: training_batches[j][1], keep_prob: 0.5})\n",
    "        batch_accuracy = sess.run(accuracy, feed_dict={x: training_batches[j][0], y_: training_batches[j][1], keep_prob: 1.0})\n",
    "        training_accuracy += batch_accuracy\n",
    "        \n",
    "        if (i + 1) / float(TRAINING_BATCH) == (i + 1) / TRAINING_BATCH:\n",
    "            print \"Epoch\",  (i + 1) / TRAINING_BATCH, \"\\n\\tTraining accuracy: {0:f}\".format(training_accuracy / TRAINING_BATCH)\n",
    "            training_accuracy = 0.0\n",
    "            np.random.shuffle(training_batches)\n",
    "            \n",
    "            ################\n",
    "            ## Validating ##\n",
    "            ################\n",
    "            \n",
    "            if RATIO != 1.0:\n",
    "                validation_accuracy = 0.0\n",
    "                for k in xrange(VALIDATION_BATCH):\n",
    "                    batch_accuracy = sess.run(accuracy, feed_dict={x: validation_batches[k][0], y_: validation_batches[k][1], keep_prob: 1.0})\n",
    "                    validation_accuracy += batch_accuracy\n",
    "\n",
    "                print \"\\tValidation accuracy: {0:f}\".format(validation_accuracy / VALIDATION_BATCH)\n",
    "                np.random.shuffle(validation_batches)\n",
    "\n",
    "        #classification = sess.run(y_conv, feed_dict={x: training_batch_images[j], keep_prob: 1.0})\n",
    "        #print classification\n",
    "        #print training_batch_labels[j]\n",
    "\n",
    "    print \"Total time:\", time.time() - t0\n",
    "\n",
    "    #save_path = saver.save(sess, \"..model/MLP_tensorflow/MLP_tensorflow.ckpt\")\n",
    "    #print(\"Model saved in file: %s\" %save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
